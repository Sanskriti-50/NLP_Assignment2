{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a9b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5dfa4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "187a50e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"preprocessed_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e06f6e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Post_ID</th>\n",
       "      <th>Comment_ID</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Majority_Sentiment</th>\n",
       "      <th>lemmatized_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>108l3ho</td>\n",
       "      <td>j3vlnat</td>\n",
       "      <td>Some of them yes but this one i got it from my...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>some of them yes but this one i got it from my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10j6oqj</td>\n",
       "      <td>j5j34zb</td>\n",
       "      <td>I keep mine in my phone cover for good luck</td>\n",
       "      <td>Positive</td>\n",
       "      <td>i keep mine in my phone cover for good luck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>10j6oqj</td>\n",
       "      <td>j5jbs3f</td>\n",
       "      <td>Hang on to it These are rare to come by as the...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>hang on to it  these are rare to come by a the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>10j6oqj</td>\n",
       "      <td>j5jfczd</td>\n",
       "      <td>Yeah Cool I got this from my classmate in 2015...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>yeah   cool  i got this from my classmate in 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>10o9tvi</td>\n",
       "      <td>j6h1ko8</td>\n",
       "      <td>Check out the Kaja Throm Beautiful marketplace...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>check out the kaja throm  beautiful marketplac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130611</th>\n",
       "      <td>130611</td>\n",
       "      <td>166838</td>\n",
       "      <td>zp46e8</td>\n",
       "      <td>0</td>\n",
       "      <td>Our population growth rate is 19 Our Birth rat...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>our population growth rate is 19  our birth ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130612</th>\n",
       "      <td>130612</td>\n",
       "      <td>166839</td>\n",
       "      <td>zp46e8</td>\n",
       "      <td>0</td>\n",
       "      <td>I obviously wont But  not only you dont do bes...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>i obviously wo nt  but  not only you do nt do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130613</th>\n",
       "      <td>130613</td>\n",
       "      <td>166840</td>\n",
       "      <td>zp46e8</td>\n",
       "      <td>0</td>\n",
       "      <td>bengal has always been some of the most fertil...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>bengal ha always been some of the most fertile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130614</th>\n",
       "      <td>130614</td>\n",
       "      <td>166841</td>\n",
       "      <td>zp46e8</td>\n",
       "      <td>0</td>\n",
       "      <td>racist agenda\\n\\nAhhh yes its always racism\\n\\...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>racist agenda ahhh yes  it s always racism  t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130615</th>\n",
       "      <td>130615</td>\n",
       "      <td>166842</td>\n",
       "      <td>zp46e8</td>\n",
       "      <td>0</td>\n",
       "      <td>Ahhh yes its always racism\\n\\nWhat else honest...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>ahhh yes  it s always racism  what else hones...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130616 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0.1  Unnamed: 0  Post_ID Comment_ID  \\\n",
       "0                  0           1  108l3ho    j3vlnat   \n",
       "1                  1           2  10j6oqj    j5j34zb   \n",
       "2                  2           3  10j6oqj    j5jbs3f   \n",
       "3                  3           4  10j6oqj    j5jfczd   \n",
       "4                  4           5  10o9tvi    j6h1ko8   \n",
       "...              ...         ...      ...        ...   \n",
       "130611        130611      166838   zp46e8          0   \n",
       "130612        130612      166839   zp46e8          0   \n",
       "130613        130613      166840   zp46e8          0   \n",
       "130614        130614      166841   zp46e8          0   \n",
       "130615        130615      166842   zp46e8          0   \n",
       "\n",
       "                                                  Comment Majority_Sentiment  \\\n",
       "0       Some of them yes but this one i got it from my...            Neutral   \n",
       "1             I keep mine in my phone cover for good luck           Positive   \n",
       "2       Hang on to it These are rare to come by as the...            Neutral   \n",
       "3       Yeah Cool I got this from my classmate in 2015...           Positive   \n",
       "4       Check out the Kaja Throm Beautiful marketplace...           Positive   \n",
       "...                                                   ...                ...   \n",
       "130611  Our population growth rate is 19 Our Birth rat...            Neutral   \n",
       "130612  I obviously wont But  not only you dont do bes...           Negative   \n",
       "130613  bengal has always been some of the most fertil...           Positive   \n",
       "130614  racist agenda\\n\\nAhhh yes its always racism\\n\\...           Negative   \n",
       "130615  Ahhh yes its always racism\\n\\nWhat else honest...           Negative   \n",
       "\n",
       "                                      lemmatized_comments  \n",
       "0       some of them yes but this one i got it from my...  \n",
       "1             i keep mine in my phone cover for good luck  \n",
       "2       hang on to it  these are rare to come by a the...  \n",
       "3       yeah   cool  i got this from my classmate in 2...  \n",
       "4       check out the kaja throm  beautiful marketplac...  \n",
       "...                                                   ...  \n",
       "130611  our population growth rate is 19  our birth ra...  \n",
       "130612  i obviously wo nt  but  not only you do nt do ...  \n",
       "130613  bengal ha always been some of the most fertile...  \n",
       "130614   racist agenda ahhh yes  it s always racism  t...  \n",
       "130615   ahhh yes  it s always racism  what else hones...  \n",
       "\n",
       "[130616 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1bb95b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_sents=[sentence for sentence in df.loc[:,\"lemmatized_comments\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "914df5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sen in proc_sents:\n",
    "    if 'http' in sen:\n",
    "        proc_sents.remove(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08e456cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['some',\n",
       "  'of',\n",
       "  'them',\n",
       "  'yes',\n",
       "  'but',\n",
       "  'this',\n",
       "  'one',\n",
       "  'i',\n",
       "  'got',\n",
       "  'it',\n",
       "  'from',\n",
       "  'my',\n",
       "  'brother',\n",
       "  'who',\n",
       "  'studied',\n",
       "  'in',\n",
       "  'malaysia',\n",
       "  'and',\n",
       "  'he',\n",
       "  'made',\n",
       "  'a',\n",
       "  'friend',\n",
       "  'from',\n",
       "  'bhutan',\n",
       "  'there',\n",
       "  'and',\n",
       "  'he',\n",
       "  'found',\n",
       "  'it',\n",
       "  'for',\n",
       "  'me'],\n",
       " ['i', 'keep', 'mine', 'in', 'my', 'phone', 'cover', 'for', 'good', 'luck']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_sents[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2965e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(proc_sents)):\n",
    "    sentence=proc_sents[i]\n",
    "    proc_sents[i]=sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03263bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['some',\n",
       "  'of',\n",
       "  'them',\n",
       "  'yes',\n",
       "  'but',\n",
       "  'this',\n",
       "  'one',\n",
       "  'i',\n",
       "  'got',\n",
       "  'it',\n",
       "  'from',\n",
       "  'my',\n",
       "  'brother',\n",
       "  'who',\n",
       "  'studied',\n",
       "  'in',\n",
       "  'malaysia',\n",
       "  'and',\n",
       "  'he',\n",
       "  'made',\n",
       "  'a',\n",
       "  'friend',\n",
       "  'from',\n",
       "  'bhutan',\n",
       "  'there',\n",
       "  'and',\n",
       "  'he',\n",
       "  'found',\n",
       "  'it',\n",
       "  'for',\n",
       "  'me'],\n",
       " ['i', 'keep', 'mine', 'in', 'my', 'phone', 'cover', 'for', 'good', 'luck']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_sents[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1803d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "train_set, validation_set = train_test_split(proc_sents, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0116675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_int = {}\n",
    "count = 0\n",
    "for sentence in train_set:\n",
    "    for token in sentence:\n",
    "        if(token not in word_to_int): \n",
    "            word_to_int[token] = count\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "108090c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = {}\n",
    "bigrams = {}\n",
    "trigrams = {}\n",
    "quadgrams = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1939bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in train_set:\n",
    "    for token in sentence:\n",
    "        if(token not in unigrams): unigrams[token] = 0\n",
    "        unigrams[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36b5a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'since': 0, 'you': 1, 'are': 2}\n"
     ]
    }
   ],
   "source": [
    "# word_to_int\n",
    "print({k: word_to_int[k] for k in list(word_to_int)[:3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f0c3ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in train_set:\n",
    "    for i in range(3 , len(sentence)):\n",
    "        quadgram = (sentence[i-3] , sentence[i-2] , sentence[i-1], sentence[i])\n",
    "        if(quadgram not in quadgrams): quadgrams[quadgram] = 0\n",
    "        quadgrams[quadgram] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9dea3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in train_set:\n",
    "    for i in range(2 , len(sentence)):\n",
    "        trigram = (sentence[i-2] , sentence[i-1], sentence[i])\n",
    "        if(trigram not in trigrams): trigrams[trigram] = 0\n",
    "        trigrams[trigram] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ecd768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in train_set:\n",
    "    for i in range(1 , len(sentence)):\n",
    "        bigram = (sentence[i-1], sentence[i])\n",
    "        if(bigram not in bigrams): bigrams[bigram] = 0\n",
    "        bigrams[bigram] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "497e6fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'since': 1979, 'you': 52791, 'are': 32857}\n"
     ]
    }
   ],
   "source": [
    "# unigrams\n",
    "print({k: unigrams[k] for k in list(unigrams)[:3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "606d6be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('since', 'you'): 134, ('you', 'are'): 3861, ('are', 'bringing'): 17}\n"
     ]
    }
   ],
   "source": [
    "# bigrams\n",
    "print({k: bigrams[k] for k in list(bigrams)[:3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9ef2eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('since', 'you', 'are'): 18, ('you', 'are', 'bringing'): 7, ('are', 'bringing', 'up'): 4}\n"
     ]
    }
   ],
   "source": [
    "# trigrams\n",
    "print({k: trigrams[k] for k in list(trigrams)[:3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f53de3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('since', 'you', 'are', 'bringing'): 1, ('you', 'are', 'bringing', 'up'): 3, ('are', 'bringing', 'up', 'different'): 1}\n"
     ]
    }
   ],
   "source": [
    "# quadgrams\n",
    "print({k: quadgrams[k] for k in list(quadgrams)[:3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e3a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54827ecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average perplexity of bigram is:  592.2785718972964\n"
     ]
    }
   ],
   "source": [
    "## Bigrams\n",
    "total_perp = 0\n",
    "count_sent = 0\n",
    "\n",
    "for sentence in validation_set:\n",
    "    \n",
    "    count_sent += 1\n",
    "    if(len(sentence) < 2): continue\n",
    "    else:\n",
    "        perp = 1\n",
    "        n = len(sentence)\n",
    "        log_perp = 0\n",
    "        for i in range(1 , n):\n",
    "            n_gram = (sentence[i-1], sentence[i])\n",
    "            \n",
    "            if(n_gram[0] not in unigrams) or (n_gram not in bigrams):\n",
    "                log_perp += math.log(len(unigrams))\n",
    "                \n",
    "            else:\n",
    "                log_perp += math.log(unigrams[n_gram[0]]/(bigrams[n_gram]))\n",
    "        log_perp = (1/n)*log_perp\n",
    "        total_perp += math.exp(log_perp)\n",
    "print(\"average perplexity of bigram is: \",total_perp / count_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8efb7047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average perplexity of Laplace smoothened bigram is:  2950.6115686115413\n"
     ]
    }
   ],
   "source": [
    "## Bigram Laplace smoothing\n",
    "\n",
    "total_perp = 0\n",
    "count_sent = 0\n",
    "\n",
    "for sentence in validation_set:\n",
    "#     print(sentence)\n",
    "    \n",
    "    count_sent += 1\n",
    "    if(len(sentence) < 2): continue\n",
    "    else:\n",
    "        perp = 1\n",
    "        n = len(sentence)\n",
    "        log_perp = 0\n",
    "        for i in range(1 , n):\n",
    "            n_gram = (sentence[i-1], sentence[i])\n",
    "            \n",
    "            smooth_prob = (unigrams.get(n_gram[0],0) + len(unigrams))/(bigrams.get(n_gram,0)+1)\n",
    "            log_perp += math.log(smooth_prob)\n",
    "\n",
    "        log_perp = (1/n)*log_perp\n",
    "\n",
    "        total_perp += math.exp(log_perp)\n",
    "print(\"average perplexity of Laplace smoothened bigram is: \",total_perp / count_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc19ff62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average perplexity of trigram is:  1973.644316192001\n"
     ]
    }
   ],
   "source": [
    "## Trigram\n",
    "total_perp = 0\n",
    "count_sent = 0\n",
    "\n",
    "for sentence in validation_set:\n",
    "    count_sent += 1\n",
    "    if(len(sentence) < 3): continue\n",
    "    else:\n",
    "        n = len(sentence)\n",
    "        log_perp = 0\n",
    "        for i in range(2 , n):\n",
    "            n_gram = (sentence[i-2], sentence[i-1], sentence[i])\n",
    "            \n",
    "            if(n_gram[:-1] not in bigrams) or (n_gram not in trigrams):\n",
    "                log_perp += math.log(len(unigrams))\n",
    "\n",
    "            else:\n",
    "                log_perp += math.log(bigrams[n_gram[:-1]]/(trigrams[n_gram]))\n",
    "        log_perp = (1/n)*log_perp\n",
    "        total_perp += math.exp(log_perp)\n",
    "print(\"average perplexity of trigram is: \",total_perp / count_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7b4a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11c23bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average perplexity of Laplace smoothened trigram is:  10617.441382254596\n"
     ]
    }
   ],
   "source": [
    "## Trigram LAPLAACE SMOOTHING\n",
    "\n",
    "total_perp = 0\n",
    "count_sent = 0\n",
    "\n",
    "for sentence in validation_set:\n",
    "    \n",
    "    count_sent += 1\n",
    "    if(len(sentence) < 3): continue\n",
    "    else:\n",
    "        n = len(sentence)\n",
    "        log_perp = 0\n",
    "        for i in range(2 , n):\n",
    "            n_gram = (sentence[i-2], sentence[i-1], sentence[i])\n",
    "            \n",
    "            smooth_prob = (bigrams.get(n_gram[:-1],0) + len(unigrams))/(trigrams.get(n_gram,0)+1)\n",
    "            log_perp += math.log(smooth_prob)\n",
    "\n",
    "        log_perp = (1/n)*log_perp\n",
    "\n",
    "        total_perp += math.exp(log_perp)\n",
    "print(\"average perplexity of Laplace smoothened trigram is: \",total_perp / count_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e208898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72406"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6694e73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average perplexity of quadgram is:  6130.475783783798\n"
     ]
    }
   ],
   "source": [
    "## Quadgram\n",
    "total_perp = 0\n",
    "count_sent = 0\n",
    "\n",
    "for sentence in validation_set:\n",
    "#     print(sentence)\n",
    "    count_sent += 1\n",
    "    if(len(sentence) < 4): continue\n",
    "    else:\n",
    "        n = len(sentence)\n",
    "        log_perp = 0\n",
    "        for i in range(3 , n):\n",
    "            n_gram = (sentence[i-3], sentence[i-2], sentence[i-1], sentence[i])\n",
    "            \n",
    "            if(n_gram[:-1] not in trigrams) or (n_gram not in quadgrams):\n",
    "                log_perp += math.log(len(unigrams)) # 7 to 6\n",
    "#                 print(len(quadgrams))\n",
    "\n",
    "            else:\n",
    "                log_perp += math.log(trigrams[n_gram[:-1]]/(quadgrams[n_gram]))\n",
    "#                 print(trigrams[n_gram[:-1]]/(quadgrams[n_gram]), trigrams[n_gram[:-1]] , (quadgrams[n_gram]) )\n",
    "        log_perp = (1/n)*log_perp\n",
    "        total_perp += math.exp(log_perp)\n",
    "print(\"average perplexity of quadgram is: \",total_perp / count_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da02fbfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d051aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average perplexity of Laplace smoothened quadgram is:  13924.504467870463\n"
     ]
    }
   ],
   "source": [
    "## Quadgram LAPLAACE SMOOTHING\n",
    "\n",
    "total_perp = 0\n",
    "count_sent = 0\n",
    "\n",
    "for sentence in validation_set:\n",
    "#     print(sentence)\n",
    "    \n",
    "    count_sent += 1\n",
    "    if(len(sentence) < 4): continue\n",
    "    else:\n",
    "        n = len(sentence)\n",
    "        log_perp = 0\n",
    "        for i in range(3 , n):\n",
    "            n_gram = (sentence[i-3], sentence[i-2], sentence[i-1], sentence[i])\n",
    "            \n",
    "            smooth_prob = (trigrams.get(n_gram[:-1],0) + len(unigrams))/(quadgrams.get(n_gram,0)+1)\n",
    "            log_perp += math.log(smooth_prob)\n",
    "\n",
    "        log_perp = (1/n)*log_perp\n",
    "\n",
    "        total_perp += math.exp(log_perp)\n",
    "print(\"average perplexity of Laplace smoothened quadgram is: \",total_perp / count_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f4b2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6d2771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
